import numpy as np
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, LSTM, Dense

# Load the data
texts = np.load('texts.npy')
labels = np.load('labels.npy')

# Tokenize the texts
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# Pad the sequences
max_length = max([len(seq) for seq in sequences])
padded_sequences = pad_sequences(sequences, maxlen=max_length)

# Split the data into train and test sets
train_data = padded_sequences[:int(len(texts) * 0.8)]
train_labels = labels[:int(len(texts) * 0.8)]
test_data = padded_sequences[int(len(texts) * 0.8):]
test_labels = labels[int(len(texts) * 0.8):]

# Initialize the model
model = Sequential()

# Add the embedding layer
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_length))

# Add the LSTM layer
model.add(LSTM(units=32))

# Add the output layer
model.add(Dense(units=1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Fit the model to the training data
model.fit(train_data, train_labels, epochs=10, batch_size=32)

# Evaluate the model on the test data
test_loss, test_acc = model.evaluate(test_data, test_labels)
print('Test accuracy:', test_acc)
